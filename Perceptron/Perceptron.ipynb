{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Multilayer Perceptron Neural Network Model\n",
    "\n",
    "This notebook uses the MNIST dataset from [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/), which consists of black and white photographs of handwritten digits. Each image is 28x28 pixels, or 784 individual values (or \"features\").\n",
    "\n",
    "The \"Label\" is a numeric value between 0 and 9, which represents the digit being drawn.\n",
    "\n",
    "This notebook uses MXNet libraries to create a four-layer neural network to detect digits. The first layer is the Input layer, the second and third layers are hidden, and the final layer is the output layer.\n",
    "\n",
    "Note: Use the 'perceptron-environment.yml' file to create the kernel environment. (Right click and choose 'Build Conda Environment'). Then choose the Kernel called \"perceptron-env'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd\n",
    "\n",
    "from IPython.display import HTML\n",
    "import cv2\n",
    "import base64\n",
    "\n",
    "print(\"Dependencies imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify whether to use a CPU or a GPU with MXNet. \n",
    "\n",
    "For Neural Networks, this is usually a GPU, but a CPU will work instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MXNet with a GPU\n",
    "#ctx = mx.gpu()\n",
    "\n",
    "# Use MXNet with a CPU\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST Dataset\n",
    "\n",
    "The following cell downloads the MNIST image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MNIST image dataset\n",
    "mnist = mx.test_utils.get_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and test data. \n",
    "\n",
    "Use the Gluon DataLoader to iterate through the dataset in mini-batches. This means validating the convergence of the learning every 'mini-batch'. (Alternatives are to validate at the end of the entire dataset, or validate stochastically at the end of each data item.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training data and test data\n",
    "\n",
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "\n",
    "# Mini-Batch size: Number of images processed in a single mini-batch. \n",
    "batch_size = 64\n",
    "\n",
    "# Create the training and test datasets.\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=transform),batch_size, shuffle=True)\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Parameters for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of inputs in a single image (28x28 pixels)\n",
    "num_inputs = 784\n",
    "\n",
    "# Number of outputs to be predicted by the network (digits 0-9)\n",
    "num_outputs = 10\n",
    "\n",
    "# Number of hidden neurons in each hidden layer\n",
    "num_hidden = 256\n",
    "\n",
    "# Weights scale defines the initial weighting for inputs into each neuron\n",
    "weight_scale = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network Layers\n",
    "\n",
    "Define the layers for our neural network. For each layer we define the weights and bias.\n",
    "\n",
    "The first input layer has input of 'num_input' (784) pixels, representing the 28x28 pixel matrix, and outputs 'num_hidden' (256) values to the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate weights and bias for the first layer\n",
    "w_hd_1 = nd.random_normal(shape=(num_inputs, num_hidden), scale=weight_scale, ctx=ctx)\n",
    "b_hd_1 = nd.random_normal(shape=num_hidden, scale=weight_scale, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second layer - a hidden layer - has input of 'num_hidden' pixels, and outputs 'num_hidden' values to the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate weights and bias for the second layer\n",
    "w_hd_2 = nd.random_normal(shape=(num_hidden, num_hidden), scale=weight_scale, ctx=ctx)\n",
    "b_hd_2 = nd.random_normal(shape=num_hidden, scale=weight_scale, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third layer - a hidden layer - also has input of 'num_hidden' pixels, and outputs 'num_hidden' values to the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate weights and bias for the third layer\n",
    "w_hd_3 = nd.random_normal(shape=(num_hidden, num_hidden), scale=weight_scale, ctx=ctx)\n",
    "b_hd_3 = nd.random_normal(shape=num_hidden, scale=weight_scale, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parameters for the output layer. This has 'num_hidden' inputs and 'num_outputs' (10) outputs, which represent the ten digits 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate weights and bias for the output layer\n",
    "w_output = nd.random_normal(shape=(num_hidden, num_outputs), scale=weight_scale, ctx=ctx)\n",
    "b_output = nd.random_normal(shape=num_outputs, scale=weight_scale, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the parameters to a list to be able to calculate the gradients on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parameters to calculate gradients\n",
    "params = [w_hd_1, b_hd_1, w_hd_2, b_hd_2, w_hd_3, b_hd_3, w_output, b_output]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a ReLU activation function for the hidden layer (We could use other activation functions like sigmoid or tanh if we wanted to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a ReLU activation function for the hidden layer\n",
    "def relu(X):\n",
    "    return nd.maximum(X, nd.zeros_like(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the network is an array that predicts digits 0-9. The softmax action function for the output layer converts these into the probabilities of an image being of a particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a softmax action function for the output layer\n",
    "def softmax_cross_entropy(yhat_linear, y):\n",
    "    return - nd.nansum(y * nd.log_softmax(yhat_linear), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Artificial Neural Network Model\n",
    "\n",
    "Build the artificial neural network by gluing together the levels defined above, add an optimizer to learn the weights and biases, and the evaluation metric to evaluate how the model is converging to a solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "def net(X):\n",
    "\n",
    "    #  Compute the first layer\n",
    "    h1_linear = nd.dot(X, w_hd_1) + b_hd_1\n",
    "    h1 = relu(h1_linear)\n",
    "\n",
    "    #  Compute the second hidden layer\n",
    "    h2_linear = nd.dot(h1, w_hd_2) + b_hd_2\n",
    "    h2 = relu(h2_linear)\n",
    "\n",
    "    #  Compute the third hidden layer\n",
    "    h3_linear = nd.dot(h2, w_hd_3) + b_hd_3\n",
    "    h3 = relu(h3_linear)\n",
    "\n",
    "    #  Compute the output layer\n",
    "    yhat_linear = nd.dot(h3, w_output) + b_output\n",
    "    return yhat_linear\n",
    "\n",
    "print(\"Neural Network defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Stochastic Gradient Descent optimizer to learn the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation metric\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        numerator += nd.sum(predictions == label)\n",
    "        denominator += data.shape[0]\n",
    "    return (numerator / denominator).asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "Define the hyper-parameters for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs: Iterations over the full network\n",
    "epochs = 10\n",
    "\n",
    "# Learning rate: Speed at which the network learns\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define a smooth constant for the moving loss\n",
    "smoothing_constant = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network model\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "\n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the results\n",
    "\n",
    "Pick some random data points from the test set and visualize them with their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function to do prediction\n",
    "def model_predict(net,data):\n",
    "    output = net(data)\n",
    "    return nd.argmax(output, axis=1)\n",
    "\n",
    "# Sample 10 random data points from the test set\n",
    "samples = 10\n",
    "\n",
    "mnist_test = mx.gluon.data.vision.MNIST(train=False, transform=transform)\n",
    "sample_data = mx.gluon.data.DataLoader(mnist_test, samples, shuffle=True)\n",
    "\n",
    "for i, (data, label) in enumerate(sample_data):\n",
    "    data = data.as_in_context(ctx)\n",
    "    im = nd.transpose(data,(1,0,2,3))\n",
    "    im = nd.reshape(im,(28,10*28,1))\n",
    "    imtiles = nd.tile(im, (1,1,3))\n",
    "    \n",
    "    plt.imshow(imtiles.asnumpy())\n",
    "    plt.show()\n",
    "    pred=model_predict(net,data.reshape((-1,784)))\n",
    "    print('model predictions are:', pred)\n",
    "    print('true labels :', label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an HTML canvas to evaluate to the model, using the canvas.html file.\n",
    "\n",
    "Use a mouse to draw a digit on the canvas, and then click **Classify**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an HTML canvas to evaluate the model\n",
    "\n",
    "def classify(img):\n",
    "    img = base64.b64decode(img[len('data:image/png;base64,'):])\n",
    "    img = cv2.imdecode(np.fromstring(img, np.uint8),-1)\n",
    "    img = cv2.resize(img[:,:,3], (28,28))\n",
    "    img = nd.array(img).as_in_context(ctx).reshape((-1, 784)).astype(np.float32)/255\n",
    "    return int(nd.argmax(net(img), axis=1).asnumpy()[0])\n",
    "\n",
    "HTML(filename = \"canvas.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete\n",
    "\n",
    "Model completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
